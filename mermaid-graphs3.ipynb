{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Amazon Bedrock boto3 Setup\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "---\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the [`boto3` Python SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to work with [Amazon Bedrock](https://aws.amazon.com/bedrock/) Foundation Models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeedd9f-f0a3-4f8e-934d-22f6f7a89de5",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Run the cells in this section to install the packages needed by the notebooks in this workshop. ⚠️ You will see pip dependency errors, you can safely ignore these errors. ⚠️\n",
    "\n",
    "IGNORE ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108c611c-7246-45c4-9f1e-76888b5076eb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3>=1.28.57\n",
      "  Using cached boto3-1.34.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting awscli>=1.29.57\n",
      "  Using cached awscli-1.32.7-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting botocore>=1.31.57\n",
      "  Using cached botocore-1.34.7-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.28.57)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.28.57)\n",
      "  Using cached s3transfer-0.10.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting docutils<0.17,>=0.10 (from awscli>=1.29.57)\n",
      "  Using cached docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
      "Collecting PyYAML<6.1,>=3.10 (from awscli>=1.29.57)\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting colorama<0.4.5,>=0.2.5 (from awscli>=1.29.57)\n",
      "  Using cached colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting rsa<4.8,>=3.1.2 (from awscli>=1.29.57)\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore>=1.31.57)\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting urllib3<2.1,>=1.25.4 (from botocore>=1.31.57)\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore>=1.31.57)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<4.8,>=3.1.2->awscli>=1.29.57)\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached boto3-1.34.7-py3-none-any.whl (139 kB)\n",
      "Using cached awscli-1.32.7-py3-none-any.whl (4.3 MB)\n",
      "Using cached botocore-1.34.7-py3-none-any.whl (11.9 MB)\n",
      "Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Using cached s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
      "Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Installing collected packages: urllib3, six, PyYAML, pyasn1, jmespath, docutils, colorama, rsa, python-dateutil, botocore, s3transfer, boto3, awscli\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "\u001b[33m      WARNING: Cannot remove entries from nonexistent file /opt/conda/lib/python3.10/site-packages/easy-install.pth\u001b[0m\u001b[33m\n",
      "\u001b[0m      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.4.8\n",
      "    Uninstalling pyasn1-0.4.8:\n",
      "      Successfully uninstalled pyasn1-0.4.8\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 0.10.0\n",
      "    Uninstalling jmespath-0.10.0:\n",
      "      Successfully uninstalled jmespath-0.10.0\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.7.2\n",
      "    Uninstalling rsa-4.7.2:\n",
      "      Successfully uninstalled rsa-4.7.2\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.33.9\n",
      "    Uninstalling botocore-1.33.9:\n",
      "      Successfully uninstalled botocore-1.33.9\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.8.2\n",
      "    Uninstalling s3transfer-0.8.2:\n",
      "      Successfully uninstalled s3transfer-0.8.2\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.33.9\n",
      "    Uninstalling boto3-1.33.9:\n",
      "      Successfully uninstalled boto3-1.33.9\n",
      "  Attempting uninstall: awscli\n",
      "    Found existing installation: awscli 1.31.9\n",
      "    Uninstalling awscli-1.31.9:\n",
      "      Successfully uninstalled awscli-1.31.9\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.4 which is incompatible.\n",
      "jupyterlab 3.4.4 requires jupyter-server~=1.16, but you have jupyter-server 2.12.1 which is incompatible.\n",
      "jupyterlab-server 2.10.3 requires jupyter-server~=1.4, but you have jupyter-server 2.12.1 which is incompatible.\n",
      "notebook 6.5.6 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.6.0 which is incompatible.\n",
      "notebook 6.5.6 requires pyzmq<25,>=17, but you have pyzmq 25.1.2 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.2 which is incompatible.\n",
      "pyasn1-modules 0.2.8 requires pyasn1<0.5.0,>=0.4.6, but you have pyasn1 0.5.1 which is incompatible.\n",
      "sagemaker 2.199.0 requires urllib3<1.27, but you have urllib3 2.0.7 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.18.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.2 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.18.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires jupyter-client<8,>=7.3.4; python_version >= \"3\", but you have jupyter-client 8.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.1 awscli-1.32.7 boto3-1.34.7 botocore-1.34.7 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 pyasn1-0.5.1 python-dateutil-2.8.2 rsa-4.7.2 s3transfer-0.10.0 six-1.16.0 urllib3-2.0.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a834cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --quiet \\\n",
    "    langchain==0.0.309 \\\n",
    "    matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f645bbbb-43ef-460a-8e8e-7b06b3b3b102",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27610c0f-7de6-4440-8f76-decf30e3c5ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create the boto3 client\n",
    "\n",
    "Interaction with the Bedrock API is done via the AWS SDK for Python: [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html).\n",
    "\n",
    "Depending on your environment, you might need to customize the setup when creating your Bedrock service client. To help with this, we've provided a `get_bedrock_client()` utility method that supports passing in different options. You can find the implementation in [../utils/bedrock.py](../utils/bedrock.py)\n",
    "\n",
    "#### Use different clients\n",
    "The boto3 provides different clients for Amazon Bedrock to perform different actions. The actions for [`InvokeModel`](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) and [`InvokeModelWithResponseStream`](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html) are supported by Amazon Bedrock Runtime where as other operations, such as [ListFoundationModels](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_ListFoundationModels.html), are handled via [Amazon Bedrock client](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock.html).\n",
    "\n",
    "The `get_bedrock_client()` method accepts `runtime` (default=True) parameter to return either `bedrock` or `bedrock-runtime` client.\n",
    "\n",
    "#### Use the default credential chain\n",
    "\n",
    "If you are running this notebook from [Amazon Sagemaker Studio](https://aws.amazon.com/sagemaker/studio/) and your Sagemaker Studio [execution role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) has permissions to access Bedrock you can just run the cells below as-is. This is also the case if you are running these notebooks from a computer whose default AWS credentials have access to Bedrock.\n",
    "\n",
    "#### Use a different AWS Region\n",
    "\n",
    "If you're running this notebook from your own computer or a SageMaker notebook in a different AWS Region from where Bedrock is set up, you can un-comment the `os.environ['AWS_DEFAULT_REGION']` line below and specify the region to use.\n",
    "\n",
    "#### Use a specific profile\n",
    "\n",
    "In case you're running this notebook from your own computer where you have setup the AWS CLI with multiple profiles, and the profile which has access to Bedrock is not the default one, you can un-comment the `os.environ['AWS_PROFILE']` line below and specify the profile to use.\n",
    "\n",
    "#### Use a different role\n",
    "\n",
    "In case you or your company has setup a specific, separate [IAM Role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) to access Bedrock, you can specify it by un-commenting the `os.environ['BEDROCK_ASSUME_ROLE']` line below. Ensure that your current user or role have permissions to [assume](https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html) such role.\n",
    "\n",
    "#### A note about `langchain`\n",
    "\n",
    "The Bedrock classes provided by `langchain` create a Bedrock boto3 client by default. To customize your Bedrock configuration, we recommend to explicitly create the Bedrock client using the method below, and pass it to the [`langchain.Bedrock`](https://python.langchain.com/docs/integrations/llms/bedrock) class instantiation method using `client=boto3_bedrock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae2b2a05-78a9-40ca-9b5e-121030f9ede1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "import bedrock\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f4adca-cfc4-439b-84b7-e528398684e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Common inference parameter definitions\n",
    "\n",
    "### Randomness and Diversity\n",
    "\n",
    "Foundation models support the following parameters to control randomness and diversity in the \n",
    "response.\n",
    "\n",
    "**Temperature** – Large language models use probability to construct the words in a sequence. For any \n",
    "given next word, there is a probability distribution of options for the next word in the sequence. When \n",
    "you set the temperature closer to zero, the model tends to select the higher-probability words. When \n",
    "you set the temperature further away from zero, the model may select a lower-probability word.\n",
    "\n",
    "In technical terms, the temperature modulates the probability density function for the next tokens, \n",
    "implementing the temperature sampling technique. This parameter can deepen or flatten the density \n",
    "function curve. A lower value results in a steeper curve with more deterministic responses, and a higher \n",
    "value results in a flatter curve with more random responses.\n",
    "\n",
    "**Top K** – Temperature defines the probability distribution of potential words, and Top K defines the cut \n",
    "off where the model no longer selects the words. For example, if K=50, the model selects from 50 of the \n",
    "most probable words that could be next in a given sequence. This reduces the probability that an unusual \n",
    "word gets selected next in a sequence.\n",
    "In technical terms, Top K is the number of the highest-probability vocabulary tokens to keep for Top-\n",
    "K-filtering - This limits the distribution of probable tokens, so the model chooses one of the highest-\n",
    "probability tokens.\n",
    "\n",
    "**Top P** – Top P defines a cut off based on the sum of probabilities of the potential choices. If you set Top \n",
    "P below 1.0, the model considers the most probable options and ignores less probable ones. Top P is \n",
    "similar to Top K, but instead of capping the number of choices, it caps choices based on the sum of their \n",
    "probabilities.\n",
    "For the example prompt \"I hear the hoof beats of ,\" you may want the model to provide \"horses,\" \n",
    "\"zebras\" or \"unicorns\" as the next word. If you set the temperature to its maximum, without capping \n",
    "Top K or Top P, you increase the probability of getting unusual results such as \"unicorns.\" If you set the \n",
    "temperature to 0, you increase the probability of \"horses.\" If you set a high temperature and set Top K or \n",
    "Top P to the maximum, you increase the probability of \"horses\" or \"zebras,\" and decrease the probability \n",
    "of \"unicorns.\"\n",
    "\n",
    "### Length\n",
    "\n",
    "The following parameters control the length of the generated response.\n",
    "\n",
    "**Response length** – Configures the minimum and maximum number of tokens to use in the generated \n",
    "response.\n",
    "\n",
    "**Length penalty** – Length penalty optimizes the model to be more concise in its output by penalizing \n",
    "longer responses. Length penalty differs from response length as the response length is a hard cut off for \n",
    "the minimum or maximum response length.\n",
    "\n",
    "In technical terms, the length penalty penalizes the model exponentially for lengthy responses. 0.0 \n",
    "means no penalty. Set a value less than 0.0 for the model to generate longer sequences, or set a value \n",
    "greater than 0.0 for the model to produce shorter sequences.\n",
    "\n",
    "### Repetitions\n",
    "\n",
    "The following parameters help control repetition in the generated response.\n",
    "\n",
    "**Repetition penalty (presence penalty)** – Prevents repetitions of the same words (tokens) in responses. \n",
    "1.0 means no penalty. Greater than 1.0 decreases repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d6999d29-94a1-4714-bb46-c9e155821212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# return the text from an html page\n",
    "def get_html_text(url, postprocess=False, print_text=False):\n",
    "\n",
    "    html = urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    if postprocess is True:\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    if print_text is True:\n",
    "        print(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# parsing completion\n",
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "\n",
    "def display_graph(graph, show_link=False):\n",
    "    graphbytes = graph.encode(\"utf8\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    url_rendering = \"https://mermaid.ink/img/\" + base64_string\n",
    "    if show_link is True:\n",
    "        print(url_rendering)\n",
    "    rendered_graph = Image(url=url_rendering)\n",
    "    display(rendered_graph)\n",
    "\n",
    "    \n",
    "def process_completion(llm_completion):\n",
    "    str_mermaid_graph = find_between(llm_completion, \"<mermaid>\", \"</mermaid>\")\n",
    "    print(str_mermaid_graph)\n",
    "    if check_graph_validity(str_mermaid_graph) is True:\n",
    "        display_graph(str_mermaid_graph)\n",
    "    else:\n",
    "        print(\"!!!The diagram has syntax issues!!!\")\n",
    "    \n",
    "    \n",
    "def check_graph_validity(graph):\n",
    "    graphbytes = graph.encode(\"utf8\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    url = \"https://mermaid.ink/img/\" + base64_string\n",
    "    req = Request(url, headers={'User-Agent' : \"Magic Browser\"})\n",
    "    \n",
    "    flag_valid = True\n",
    "    try: \n",
    "        con = urlopen(req)\n",
    "    except:\n",
    "        flag_valid = False\n",
    "    \n",
    "    return flag_valid\n",
    "        \n",
    "\n",
    "    \n",
    "def standardize_graph(graph):\n",
    "    \n",
    "    # double to single\n",
    "    graph = graph.replace(\"((\", \"(\")\n",
    "    graph = graph.replace(\"))\", \")\")\n",
    "    graph = graph.replace(\"{{\", \"{\")\n",
    "    graph = graph.replace(\"}}\", \"}\")\n",
    "    graph = graph.replace(\"[[\", \"[\")\n",
    "    graph = graph.replace(\"]]\", \"]\")\n",
    "    graph = graph.replace(\"||\", \"|\")\n",
    "    \n",
    "    graph = graph.replace(\"([\", \"(\")\n",
    "    graph = graph.replace(\"])\", \")\")\n",
    "    graph = graph.replace(\"[(\", \"[\")\n",
    "    graph = graph.replace(\")]\", \"]\")\n",
    "    graph = graph.replace(\"({\", \"(\")\n",
    "    graph = graph.replace(\"})\", \")\")\n",
    "    graph = graph.replace(\"{(\", \"{\")\n",
    "    graph = graph.replace(\")}\", \"}\")\n",
    "    graph = graph.replace(\"{[\", \"{\")\n",
    "    graph = graph.replace(\"]}\", \"}\")\n",
    "    graph = graph.replace(\"[{\", \"[\")\n",
    "    graph = graph.replace(\"}]\", \"]\")\n",
    "    \n",
    "    # remove quotes if they exist (to be added later)\n",
    "    graph = graph.replace('(\"', '(')\n",
    "    graph = graph.replace('\")', ')')\n",
    "    graph = graph.replace('[\"', '[')\n",
    "    graph = graph.replace('\"]', ']')\n",
    "    graph = graph.replace('{\"', '{')\n",
    "    graph = graph.replace('\"}', '}')\n",
    "    \n",
    "    # add quotes\n",
    "    graph = graph.replace('(', '(\"')\n",
    "    graph = graph.replace(')', '\")')\n",
    "    graph = graph.replace('[', '[\"')\n",
    "    graph = graph.replace(']', '\"]')\n",
    "    graph = graph.replace('{', '{\"')\n",
    "    graph = graph.replace('}', '\"}')\n",
    "    \n",
    "    # # remove spaces in arrows (to be added later)\n",
    "    # graph = graph.replace(' -->', '-->')\n",
    "    # graph = graph.replace('--> ', '-->')\n",
    "    # graph = graph.replace('-->', ' --> ')\n",
    "    \n",
    "    # remove unsafe caracters for base64 encoding\n",
    "    graph = graph.replace('/', '')\n",
    "    graph = graph.replace('+', '')\n",
    "    \n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a6ba812e-35fd-4372-88a5-46e327405e78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_diagram(\n",
    "    url,\n",
    "    number_of_diagrams=1,\n",
    "    kind=\"flowchart\", # \"mindmap\" or \"flowchart\"\n",
    "    orientation=\"LR\", # \"LR\" or \"TD\"\n",
    "    mermaid_context=True,\n",
    "    max_tokens_to_sample=500,\n",
    "    temperature=0.9,\n",
    "    top_k=250,\n",
    "    top_p=1,\n",
    "    show_raw_result=True,\n",
    "    display=True\n",
    "):\n",
    "    ls_diagrams=[]\n",
    "    \n",
    "    for d in range(number_of_diagrams):\n",
    "        print(\"--- Generating diagram\", d+1, \"out of\", number_of_diagrams, \"---\")\n",
    "        \n",
    "        print(\"...Getting context...\")\n",
    "        html_text = context_mermaid_notation = get_html_text(\n",
    "            url=url, \n",
    "            postprocess=False, \n",
    "            print_text=False\n",
    "        )\n",
    "\n",
    "        if mermaid_context is True:\n",
    "            context_mermaid_notation = get_html_text(\n",
    "                url=\"https://mermaid.js.org/syntax/flowchart.html\", \n",
    "                postprocess=False, \n",
    "                print_text=False\n",
    "            )\n",
    "        else:\n",
    "            context_mermaid_notation = \"\"\n",
    "\n",
    "\n",
    "        prompt = f\"\"\"\\n\\nHuman: \n",
    "        Here is a text for you to reference for the following task:\n",
    "        <text>\n",
    "        {html_text}\n",
    "        </text>\n",
    "\n",
    "        Task: Summarize the given text and provide the summary inside <summary> tags. \n",
    "        Then convert the summary to a {kind} using Mermaid notation. \n",
    "\n",
    "        <mermaid_notation>\n",
    "        {context_mermaid_notation}\n",
    "        </mermaid_notation>\n",
    "\n",
    "        The {kind} should capture the main gist of the summary, without too many low-level details. \n",
    "        Someone who would only view the Mermaid {kind}, should understand the gist of the summary. \n",
    "        The Mermaid {kind} should follow all the correct notation rules and should compile without any errors.\n",
    "        Use the following specifications for the generated Mermaid {kind}:\n",
    "\n",
    "        <specifications>\n",
    "        1. Use different colors, shapes or groups to represent different concepts in the given text.\n",
    "        2. The orientation of the Mermaid {kind} should be {orientation}.\n",
    "        3. Any text inside parenthesis (), square brackets [], curly brackets {{}}, or bars ||, should be inside quotes \"\".\n",
    "        4. Include the Mermaid {kind} inside <mermaid> tags.\n",
    "        5. Do not write anything after the </mermaid> tag.\n",
    "        6. Use only information from within the given text. Don't make up new information.\n",
    "        </specifications>\n",
    "\n",
    "        \\n\\nAssistant:\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = prompt.replace(\"{{}}\", \"{}\")\n",
    "        \n",
    "        print(\"...LLM is generating...\") \n",
    "        body = json.dumps(\n",
    "            {\n",
    "                \"prompt\": prompt, \n",
    "                \"max_tokens_to_sample\": max_tokens_to_sample,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_k\": top_k,\n",
    "                \"top_p\": top_p,\n",
    "                \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "            }\n",
    "        )\n",
    "        modelId = \"anthropic.claude-v2:1\"  # change this to use a different version from the model provider\n",
    "        accept = \"application/json\"\n",
    "        contentType = \"application/json\"\n",
    "\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "                body=body, \n",
    "                modelId=modelId, \n",
    "                accept=accept, \n",
    "                contentType=contentType\n",
    "            )\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "        print(\"...Displaying graph...\")\n",
    "\n",
    "        if show_raw_result is True:\n",
    "            print(response_body.get(\"completion\"))\n",
    "\n",
    "        str_mermaid_graph = find_between(\n",
    "            response_body.get(\"completion\"), \n",
    "            \"<mermaid>\", \n",
    "            \"</mermaid>\"\n",
    "        )\n",
    "        \n",
    "        dc_output = {}\n",
    "        dc_output[\"raw\"] = response_body.get(\"completion\")\n",
    "        dc_output[\"graph\"] = str_mermaid_graph\n",
    "        dc_output[\"standardized_graph\"] = standardize_graph(str_mermaid_graph)\n",
    "        # dc_output[\"graph\"] = str_mermaid_graph.replace(\",\", \"\")  # beause https://mermaid.ink/ raises an error! (why?)\n",
    "        dc_output[\"valid\"] = check_graph_validity(str_mermaid_graph)\n",
    "        ls_diagrams.append(dc_output)\n",
    "        \n",
    "        if display is True:\n",
    "            print(dc_output[\"standardized_graph\"])\n",
    "            if check_graph_validity(dc_output[\"standardized_graph\"]) is True:\n",
    "                display_graph(dc_output[\"standardized_graph\"])\n",
    "            else:\n",
    "                print(\"!!!The diagram has syntax issues!!!\\n\")\n",
    "    \n",
    "    return ls_diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "961527d7-6dcd-4ff9-bf10-0eaa81046240",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating diagram 1 out of 3 ---\n",
      "...Getting context...\n",
      "...LLM is generating...\n",
      "...Displaying graph...\n",
      "\n",
      "flowchart LR\n",
      "    direction LR\n",
      "    C1[\"Custom model with <br>custom framework\"] --> A1(\"Build <br>own container\")\n",
      "    C2[\"Custom model <br>needing custom <br>package\"] --> B2[\"Extend built-in <br>container\"]\n",
      "    C3[\"Custom model <br>supported <br>framework\"] --> B1{\"Use built-in <br>container as is\"}\n",
      "    C4[\"Pre-built <br>algorithm model\"] --> B3[\"Use pre-built <br>container\"]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmZsb3djaGFydCBMUgogICAgZGlyZWN0aW9uIExSCiAgICBDMVsiQ3VzdG9tIG1vZGVsIHdpdGggPGJyPmN1c3RvbSBmcmFtZXdvcmsiXSAtLT4gQTEoIkJ1aWxkIDxicj5vd24gY29udGFpbmVyIikKICAgIEMyWyJDdXN0b20gbW9kZWwgPGJyPm5lZWRpbmcgY3VzdG9tIDxicj5wYWNrYWdlIl0gLS0+IEIyWyJFeHRlbmQgYnVpbHQtaW4gPGJyPmNvbnRhaW5lciJdCiAgICBDM1siQ3VzdG9tIG1vZGVsIDxicj5zdXBwb3J0ZWQgPGJyPmZyYW1ld29yayJdIC0tPiBCMXsiVXNlIGJ1aWx0LWluIDxicj5jb250YWluZXIgYXMgaXMifQogICAgQzRbIlByZS1idWlsdCA8YnI+YWxnb3JpdGhtIG1vZGVsIl0gLS0+IEIzWyJVc2UgcHJlLWJ1aWx0IDxicj5jb250YWluZXIiXQo=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating diagram 2 out of 3 ---\n",
      "...Getting context...\n",
      "...LLM is generating...\n",
      "...Displaying graph...\n",
      "\n",
      "flowchart LR\n",
      "    direction LR\n",
      "    A[\"Use Cases\"] --> B{\"Need<br>custom<br>packages?\"}\n",
      "    B --> |No| C[\"Use pre-built<br>container\"]\n",
      "    B --> |Yes| D{\"Pre-built<br>container<br>supports<br>requirements.txt?\"} \n",
      "    D --> |Yes| E[\"Add dependencies<br>in requirements.txt\"]\n",
      "    D --> |No| F[\"Extend pre-built<br>container\"]\n",
      "    F --> C\n",
      "    C --> G[\"Train &<br>deploy<br>model\"]\n",
      "\n",
      "    classDef grey fill:#ddd,stroke:#fff,stroke-width:4px,color:#000\n",
      "    class A,G grey\n",
      "\n",
      "!!!The diagram has syntax issues!!!\n",
      "\n",
      "--- Generating diagram 3 out of 3 ---\n",
      "...Getting context...\n",
      "...LLM is generating...\n",
      "...Displaying graph...\n",
      "\n",
      "!!!The diagram has syntax issues!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ls_diagrams = generate_diagram(\n",
    "    # url=\"https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-vpc.html\",\n",
    "    url=\"https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers.html\",\n",
    "    # url=\"https://code.likeagirl.io/creating-flowcharts-with-mermaid-in-python-3cbca0058ecb\",\n",
    "    number_of_diagrams=3,\n",
    "    kind=\"flowchart\", # \"mindmap\" or \"flowchart\"\n",
    "    orientation=\"LR\", # \"LR\" or \"TD\"\n",
    "    mermaid_context=True,\n",
    "    max_tokens_to_sample=500,\n",
    "    temperature=0.9,\n",
    "    top_k=250,\n",
    "    top_p=1,\n",
    "    show_raw_result=False,\n",
    "    display=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa931cc6-5cdf-4c7c-a5b0-03d351681bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b4440123-2418-4140-8733-e20521eb7bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\nflowchart LR\\n    direction LR\\n    A[\"Use Cases\"] --> B{\"Need<br>custom<br>packages ?\"}\\n    B --> |No| C[\"Use pre-built<br>container\"]\\n    B --> |Yes| D{\"Pre-built<br>container<br>supports<br>requirements.txt?\"} \\n    D --> |Yes| E[\"Add dependencies<br>in requirements.txt\"]\\n    D --> |No| F[\"Extend pre-built<br>container\"]\\n    F --> C\\n    C --> G[\"Train &<br>deploy<br>model\"]\\n\\n    classDef grey fill:#ddd,stroke:#fff,stroke-width:4px,color:#000\\n    class A,G grey\\n'\n",
      "\n",
      "flowchart LR\n",
      "    direction LR\n",
      "    A[\"Use Cases\"] --> B{\"Need<br>custom<br>packages ?\"}\n",
      "    B --> |No| C[\"Use pre-built<br>container\"]\n",
      "    B --> |Yes| D{\"Pre-built<br>container<br>supports<br>requirements.txt?\"} \n",
      "    D --> |Yes| E[\"Add dependencies<br>in requirements.txt\"]\n",
      "    D --> |No| F[\"Extend pre-built<br>container\"]\n",
      "    F --> C\n",
      "    C --> G[\"Train &<br>deploy<br>model\"]\n",
      "\n",
      "    classDef grey fill:#ddd,stroke:#fff,stroke-width:4px,color:#000\n",
      "    class A,G grey\n",
      "\n",
      "https://mermaid.ink/img/CmZsb3djaGFydCBMUgogICAgZGlyZWN0aW9uIExSCiAgICBBWyJVc2UgQ2FzZXMiXSAtLT4gQnsiTmVlZDxicj5jdXN0b208YnI+cGFja2FnZXMgPyJ9CiAgICBCIC0tPiB8Tm98IENbIlVzZSBwcmUtYnVpbHQ8YnI+Y29udGFpbmVyIl0KICAgIEIgLS0+IHxZZXN8IER7IlByZS1idWlsdDxicj5jb250YWluZXI8YnI+c3VwcG9ydHM8YnI+cmVxdWlyZW1lbnRzLnR4dD8ifSAKICAgIEQgLS0+IHxZZXN8IEVbIkFkZCBkZXBlbmRlbmNpZXM8YnI+aW4gcmVxdWlyZW1lbnRzLnR4dCJdCiAgICBEIC0tPiB8Tm98IEZbIkV4dGVuZCBwcmUtYnVpbHQ8YnI+Y29udGFpbmVyIl0KICAgIEYgLS0+IEMKICAgIEMgLS0+IEdbIlRyYWluICY8YnI+ZGVwbG95PGJyPm1vZGVsIl0KCiAgICBjbGFzc0RlZiBncmV5IGZpbGw6I2RkZCxzdHJva2U6I2ZmZixzdHJva2Utd2lkdGg6NHB4LGNvbG9yOiMwMDAKICAgIGNsYXNzIEEsRyBncmV5Cg==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmZsb3djaGFydCBMUgogICAgZGlyZWN0aW9uIExSCiAgICBBWyJVc2UgQ2FzZXMiXSAtLT4gQnsiTmVlZDxicj5jdXN0b208YnI+cGFja2FnZXMgPyJ9CiAgICBCIC0tPiB8Tm98IENbIlVzZSBwcmUtYnVpbHQ8YnI+Y29udGFpbmVyIl0KICAgIEIgLS0+IHxZZXN8IER7IlByZS1idWlsdDxicj5jb250YWluZXI8YnI+c3VwcG9ydHM8YnI+cmVxdWlyZW1lbnRzLnR4dD8ifSAKICAgIEQgLS0+IHxZZXN8IEVbIkFkZCBkZXBlbmRlbmNpZXM8YnI+aW4gcmVxdWlyZW1lbnRzLnR4dCJdCiAgICBEIC0tPiB8Tm98IEZbIkV4dGVuZCBwcmUtYnVpbHQ8YnI+Y29udGFpbmVyIl0KICAgIEYgLS0+IEMKICAgIEMgLS0+IEdbIlRyYWluICY8YnI+ZGVwbG95PGJyPm1vZGVsIl0KCiAgICBjbGFzc0RlZiBncmV5IGZpbGw6I2RkZCxzdHJva2U6I2ZmZixzdHJva2Utd2lkdGg6NHB4LGNvbG9yOiMwMDAKICAgIGNsYXNzIEEsRyBncmV5Cg==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mermaid_code = ls_diagrams[1][\"standardized_graph\"]\n",
    "mermaid_code=mermaid_code.replace(\n",
    "    'A[\"Use Cases\"] --> B{\"Need<br>custom<br>packages?\"}', \n",
    "    'A[\"Use Cases\"] --> B{\"Need<br>custom<br>packages ?\"}'\n",
    ")\n",
    "\n",
    "# mermaid_code=mermaid_code[:mermaid_code.find('B --> |No|')]\n",
    "print(repr(mermaid_code))\n",
    "print((mermaid_code))\n",
    "\n",
    "display_graph(mermaid_code, show_link=True)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
